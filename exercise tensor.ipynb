{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 编程练习\n",
    "\n",
    "请尝试使用学到的tensor知识，完成下述问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "img = cv2.imread('template.jpg')\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tensor的创建、属性修改、索引、拼接、维度调整等"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）使用opencv读取的图像默认是BGR三通道的ndarrays格式，数据类型是uint8。基于pytorch实现的神经网络往往要求输入数据使用float32类型，请创建一个和img具有相同内容的tensor，并将其数据类型调整为float32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(img, dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）请取出该tensor表示的图像的上半部分，即对于尺寸为（48，48，3）的图像取出前24行得到尺寸为（24，48，3）的tensor，命名为img_top。类似地，请再取出图像的下半部分，得到另一个尺寸为（24，48，3）的tensor，命名为img_down。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "img_top=x[0:24,0:48,0:3]\n",
    "img_down=x[24:48,0:48,0:3]\n",
    "print(img_top)\n",
    "print(img_down)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）请将img_down置于img_top上方对二者进行拼接，得到尺寸为（48，48，3）的tensor，命名为img_cat。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cat = torch.cat([img_down, img_top], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）在pytorch实现的神经网络中，往往要求输入图像具有(batch_size, channel, height, width)的维度顺序。请将拼接后的img_cat从(height, width, channel)的维度顺序调整为(batch_size, channel, height, width)的顺序，命名为img_pytorch, 其中batch_size应为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "img_unsqueeze = img_cat.unsqueeze(-1)\n",
    "img_pytorch=img_unsqueeze.permute(3, 2, 0, 1)\n",
    "print(img_pytorch.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tensor的数学运算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）提供的模板图像是一幅黑白图像，可以很容易地根据三通道值之和是否为0来区分背景和前景。请据此得到img_pytorch的前景mask，命名为img_mask，img_mask应具有(batch_size, height, width)的维度，并且数据类型应为float32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "img_mask=torch.zeros(1,48,48,dtype=torch.float32,device='cpu')\n",
    "for i in range(48):\n",
    "    for j in  range(48):\n",
    "        count=0\n",
    "        for k in range(3): \n",
    "            count+=img_pytorch[0,k,i,j]\n",
    "        if(count!=0):\n",
    "            img_mask[0,i,j]=count\n",
    "print(img_mask)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）请对img_pytorch的前景部分添加在[-50, 0]之间均匀分布的随机噪声，结果命名为img_noise，可以使用前面获得的前景mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          ...,\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691]],\n",
      "\n",
      "         [[-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          ...,\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691]],\n",
      "\n",
      "         [[-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          ...,\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691],\n",
      "          [-27.2691, -27.2691, -27.2691,  ..., -27.2691, -27.2691, -27.2691]]]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "number = random.uniform(-50,0)\n",
    "img_noise=img_pytorch+number\n",
    "print(img_noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70f3b6f3186ac5eb7a7b44422c7e937631bceac8e358a36f9e12771b61974c1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
